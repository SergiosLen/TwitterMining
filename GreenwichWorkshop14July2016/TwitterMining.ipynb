{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining (together with a bit of web scraping) of large social networks from Twitter using Python (and Ruby)\n",
    "## By Moses Boudourides and Sergios Lenis \n",
    "## University of Patras, Greece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I. Prerequisite Python Modules and Scripts](#I)\n",
    "\n",
    "[II. Twitter Mining from the Twitter API](#II)\n",
    "\n",
    "[III. Web Scraping from the Twitter Advanced Search](#III)\n",
    "\n",
    "[IV. Statistical & Network Analyses](#IV)\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='I'></a>\n",
    "## I. Prerequisite Python Modules and Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The following cell imports the prerequisite Python modules for this network to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First, one has to download the *github* directory *https://github.com/mboudour/TwitterMining*, where everything needed for this notebook to run is included.**\n",
    "\n",
    "### **Github Blog: *http://mboudour.github.io/***\n",
    "\n",
    "### **Furthermore, one needs to have already installed all the modules imported in the script** *collect_tweets_notebook.py*. Some of these modules can be installed and imported from the notebook as follows (without #):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install python-twitter\n",
    "# import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='II'></a>\n",
    "## II. Twitter Mining from the Twitter API (https://apps.twitter.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting Input and Output Directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am being imported from another module\n"
     ]
    }
   ],
   "source": [
    "# input_dir='/Users/mosesboudourides/GithubRepositories/TwitterMining'\n",
    "input_dir='/home/mab/github_repos/TwitterMining'\n",
    "\n",
    "# output_dir='/Users/mosesboudourides/twitTemp'\n",
    "output_dir='/home/mab/Desktop/twitTemp'\n",
    "\n",
    "# cred_dic=None\n",
    "cred_dic='/home/mab/Desktop/twitTemp/credentials/auth_cred.txt'\n",
    "# cred_dic='/Users/mosesboudourides/twitTemp/credentials/auth_cred.txt'\n",
    "# cred_dic='/home/mab/Dropbox/Python Projects/EUSN2016_TwitterWorkshop/TwitterMining/credentials/auth_cred.txt'\n",
    "# cred_dic='/media/sergios-len/Elements/Brighton_workshop/auth_cred.txt'\n",
    "\n",
    "pp= !pwd\n",
    "os.chdir(input_dir)\n",
    "from test_class_tpa import create_df\n",
    "import collect_tweets_notebook as ctn\n",
    "\n",
    "os.chdir(pp[0])\n",
    "\n",
    "def create_beaker_com_dict(sps):\n",
    "    nsps={}\n",
    "    for k,v in sps.items():\n",
    "        nsps[k]=[]\n",
    "        if k=='date_split':\n",
    "            for kk in sorted(v.keys()):\n",
    "                nsps[k].append(v[kk].strftime('%Y%m%d'))\n",
    "        else:\n",
    "            for kk in sorted(v.keys()):\n",
    "                nsps[k].append(v[kk])\n",
    "\n",
    "    return nsps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authentication and login in Twitter API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv=ctn.UserAuth(auth_file=cred_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **After the authentication tokens are known, one has to insert them below by decommenting and running the following three cells:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vv.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Mon Dec 29 11:20:53 +0000 2008\", \"description\": \"\\u30e2\\u30fc\\u30bc\\u30ba\", \"favourites_count\": 1031, \"followers_count\": 1174, \"friends_count\": 420, \"geo_enabled\": true, \"id\": 18447918, \"lang\": \"en\", \"listed_count\": 144, \"location\": \"Patras, Greece\", \"name\": \"Moses Boudourides\", \"profile_background_color\": \"9C584B\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/150481468/wordpainting.jpg\", \"profile_background_tile\": true, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/18447918/1377851188\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/378800000721710479/c093b7142774b1c8a07b48a8edca8d37_normal.png\", \"profile_link_color\": \"FF0D00\", \"profile_sidebar_fill_color\": \"FFF7CC\", \"profile_text_color\": \"0C3E53\", \"screen_name\": \"mosabou\", \"status\": {\"created_at\": \"Thu Jun 30 04:43:59 +0000 2016\", \"hashtags\": [], \"id\": 748376514710863872, \"id_str\": \"748376514710863872\", \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/UDxxrXyX6q\", \"expanded_url\": \"http://twitter.com/samuelbbeckett/status/748179013944872960/photo/1\", \"id\": 748178971997642752, \"media_url\": \"http://pbs.twimg.com/media/CmIQz55WIAAfP0X.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/CmIQz55WIAAfP0X.jpg\", \"type\": \"photo\", \"url\": \"https://t.co/UDxxrXyX6q\"}], \"retweet_count\": 13, \"retweeted\": true, \"retweeted_status\": {\"created_at\": \"Wed Jun 29 15:39:12 +0000 2016\", \"favorite_count\": 16, \"hashtags\": [], \"id\": 748179013944872960, \"id_str\": \"748179013944872960\", \"lang\": \"en\", \"media\": [{\"display_url\": \"pic.twitter.com/UDxxrXyX6q\", \"expanded_url\": \"http://twitter.com/samuelbbeckett/status/748179013944872960/photo/1\", \"id\": 748178971997642752, \"media_url\": \"http://pbs.twimg.com/media/CmIQz55WIAAfP0X.jpg\", \"media_url_https\": \"https://pbs.twimg.com/media/CmIQz55WIAAfP0X.jpg\", \"type\": \"photo\", \"url\": \"https://t.co/UDxxrXyX6q\"}], \"retweet_count\": 13, \"retweeted\": true, \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \"text\": \"One does not have to look for distress. It is screaming at you even in the taxis of London.\\nhttps://t.co/8mY4ZkP7Az https://t.co/UDxxrXyX6q\", \"urls\": [{\"expanded_url\": \"https://rhystranter.com/2016/06/29/samuel-beckett-on-distress/\", \"url\": \"https://t.co/8mY4ZkP7Az\"}], \"user_mentions\": []}, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"text\": \"RT @samuelbbeckett: One does not have to look for distress. It is screaming at you even in the taxis of London.\\nhttps://t.co/8mY4ZkP7Az htt\\u2026\", \"urls\": [{\"expanded_url\": \"https://rhystranter.com/2016/06/29/samuel-beckett-on-distress/\", \"url\": \"https://t.co/8mY4ZkP7Az\"}], \"user_mentions\": [{\"id\": 262440246, \"name\": \"Samuel Beckett\", \"screen_name\": \"samuelbbeckett\"}]}, \"statuses_count\": 23010, \"time_zone\": \"Athens\", \"url\": \"https://t.co/1zll77DosO\", \"utc_offset\": 10800}\n"
     ]
    }
   ],
   "source": [
    "vv.check_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twi_api=vv.get_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up a Search**\n",
    "#### **Further info about how to build a Twitter query is available at: https://dev.twitter.com/rest/public/search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_term='@MediaGovGr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sea=ctn.TwitterSearch(twi_api,search_text=search_term,working_path=output_dir,out_file_dir=None,\n",
    "max_pages=10,results_per_page=100,sin_id=None,max_id=None,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 tweets preloaded from /home/mab/Desktop/twitTemp/Output/MediaGovGr.ids\n",
      "None ================\n",
      "18 new tweets logged at /home/mab/Desktop/twitTemp/Output/MediaGovGr.json\n",
      "1 100 aa None Mon Jun 27 09:18:50 +0000 2016\n",
      "747358516269166592 ================\n",
      "745878183619223552 ================\n",
      "57 new tweets logged at /home/mab/Desktop/twitTemp/Output/MediaGovGr.json\n",
      "3 92 aa 745878183619223552 Wed Jun 22 06:27:59 +0000 2016\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n",
      "745503580841353216 ================\n"
     ]
    },
    {
     "ename": "TwitterError",
     "evalue": "[{u'message': u'Rate limit exceeded', u'code': 88}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTwitterError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bb5077e1907f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msea\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreamsearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mcollect_tweets_notebook.pyc\u001b[0m in \u001b[0;36mstreamsearch\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;32mcollect_tweets_notebook.pyc\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;32m/home/mab/.local/lib/python2.7/site-packages/twitter/api.pyc\u001b[0m in \u001b[0;36mGetSearch\u001b[1;34m(self, term, raw_query, geocode, since_id, max_id, until, since, count, lang, locale, result_type, include_entities)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RequestUrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ParseAndCheckTwitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewFromJsonDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'statuses'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mab/.local/lib/python2.7/site-packages/twitter/api.pyc\u001b[0m in \u001b[0;36m_ParseAndCheckTwitter\u001b[1;34m(self, json_data)\u001b[0m\n\u001b[0;32m   4686\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4687\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_CheckForTwitterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4689\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4690\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"<title>Twitter / Over capacity</title>\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mab/.local/lib/python2.7/site-packages/twitter/api.pyc\u001b[0m in \u001b[0;36m_CheckForTwitterError\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   4713\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'errors'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4715\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTwitterError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4717\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_RequestChunkedUpload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTwitterError\u001b[0m: [{u'message': u'Rate limit exceeded', u'code': 88}]"
     ]
    }
   ],
   "source": [
    "sea.streamsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>To interrupt the collection of tweets initiated above, one has to click \"Kernel > Interrupt\" from the Notebook menu.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The data collected from the above search are saved as a json file in the above defined output_dir named by the above defined search_term.**\n",
    "\n",
    "### **In the json file, there are four main “objects” provided by the API:** \n",
    "* **Tweets,** \n",
    "* **Users,** \n",
    "* **Entities and** \n",
    "* **Places.**\n",
    "\n",
    "### **Definitions and info about all these ojects is given in https://dev.twitter.com/overview/api.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting 21 practically intersesting \"objects\" and creating a Pandas data frame with them as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "user_id\n",
      "username\n",
      "created_at\n",
      "language\n",
      "hashtag_count\n",
      "retweet_count\n",
      "mention_count\n",
      "statuses_count\n",
      "followers_count\n",
      "friends_count\n",
      "listed_count\n",
      "videos_count\n",
      "photos_count\n",
      "undef_count\n",
      "coordinates\n",
      "bounding\n",
      "place\n",
      "hashtags\n",
      "mentions\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "columnss=['id','user_id','username','created_at','language','hashtag_count','retweet_count','mention_count',\n",
    "          'statuses_count','followers_count','friends_count','listed_count','videos_count','photos_count',\n",
    "          'undef_count','coordinates','bounding','place','hashtags','mentions','text'] \n",
    "for i in columnss:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='III'></a>\n",
    "## III. Twitter Mining from the Twitter Advanced Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **For Twitter Scraping, we are using the *Beaker Notebook *https://pub.beakernotebook.com/publications/ee134c26-2b23-11e6-abb8-6fa10fd07640?fullscreen=true**.\n",
    "\n",
    "### **Again, one needs to have already installed all the modules imported in this *Beaker Notebook* as well in all the scripts of the *github* directory *https://github.com/mboudour/TwitterMining*.** \n",
    "\n",
    "### **First, one should start with an advanced search at *https://twitter.com/search-advanced.* **\n",
    "\n",
    "### **If the searched term is a hashtag (or multiple hashtags), one should continue with the *Beaker Notebook* *https://pub.beakernotebook.com/publications/ee134c26-2b23-11e6-abb8-6fa10fd07640?fullscreen=true*.**\n",
    "\n",
    "### **Otherwise (for non-hashtag-type search terms), one has to open the page with the outcome of the Twitter search, copy the substring in the URL that follows 'search?q=' before '&src=\" and paste it in the second cell of the *Twitter Scraping in Ruby Beaker Notebook* after 'searchterm='. In the search below, it suffices to copy the string 'day%20night%20paris'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='IV'></a>\n",
    "## IV. Statistical & Network Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **An example done in the *Beaker Notebook* https://pub.beakernotebook.com/publications/3a62f03e-27e8-11e6-9ac4-6732ff96645f?fullscreen=true**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
